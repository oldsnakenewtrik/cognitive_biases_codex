**\# Not Enough Meaning**

When humans problem solve, begin a logical reasoning or complex problem solving assignment they are faced with the situation of having, “Not Enough Meaning\!”, based on limitations to their brain processing power.

\#\# Human Information Processing Tendencies Regarding Not Enough Meaning

\#\#\# We project our current mindset and assumptions onto the past and future

\#\#\#\# Self-consistency bias

Self-consistency bias refers to the tendency of individuals to believe that their attitudes, opinions, and beliefs are more consistent over time than they actually are. This bias is also known as the commitment and self-consistency bias. It encapsulates the inclination of people to align their beliefs and behaviors with their values and self-image, often leading to a perception of greater consistency in their views and actions than is actually the case​.

\#\#\#\# Restraint bias

Restraint bias is a cognitive bias where individuals overestimate their ability to control their impulses or resist temptation. This bias manifests as an overestimation of one's capacity to control impulsive behavior, leading to an inflated belief in self-control. Such an exaggerated perception of self-restraint can result in greater exposure to temptation and, paradoxically, increased impulsiveness. Restraint bias is particularly relevant in the context of addiction, where individuals may underestimate the challenges in controlling addictive behaviors due to an overconfidence in their self-control abilities

\#\#\#\# Projection bias

Projection bias is a cognitive bias where individuals tend to overestimate the extent to which their future selves will share their current preferences, thoughts, and values. This can lead to sub-optimal choices, as it affects decision-making by assuming that future preferences will align with current ones, often disregarding the possibility of change or evolution in one's tastes, opinions, and priorities​

\#\#\#\# Pro-innovation bias

Pro-innovation bias refers to the belief that a particular innovation should be adopted by the entire society without any need for its alteration. This bias often arises when the "champion" of an innovation has such a strong belief in its benefits that they may overlook or fail to recognize its limitations or weaknesses, continuing to promote it despite potential drawbacks​​.

\#\#\#\# Time-saving bias

Time-saving bias is a cognitive concept that describes the tendency of people to inaccurately estimate the time that can be saved (or lost) when changing speed. This bias commonly leads individuals to underestimate the time savings gained from increasing speed from a low level and to overestimate the time savings when increasing speed from a high level. The bias highlights a general misjudgment in how speed changes affect time efficiency​​.

\#\#\#\# Planning fallacy

The planning fallacy is a cognitive bias that leads people to underestimate the time needed to complete future tasks. First proposed by psychologists Daniel Kahneman and Amos Tversky in 1979, this bias is characterized by reliance on overly optimistic scenarios and the neglect of past experiences with similar tasks. Essentially, individuals affected by this bias expect things to go more smoothly and quickly than they typically do, often disregarding prior experiences that suggest otherwise

\#\#\#\# Pessimism bias

Pessimism bias, also known as negativity bias, is the psychological phenomenon by which humans pay more attention to and give more weight to negative rather than positive experiences or information. It's an evolved trait that promoted survival as humans who were more attuned to threats and dangers were more likely to avoid harm. However, in modern society, pessimism bias can cause people to overestimate risks or perceive more negativity in a situation than is warranted based on objective facts. Overall, it leads to a tendency to focus more on the bad over the good across many domains.

\#\#\#\# Impact bias  
Impact bias is the tendency for people to overestimate the length or intensity of the impact of future feeling states. For example, people tend to overestimate the degree of pleasure or displeasure (respectively) of a future promotional achievement or dating mishap. Impact bias has been called emotion's oracle illusion.

\#\#\#\# Declinism

Declinism is the belief that a society or institution is on a trajectory towards decline. It often involves a cognitive bias, such as rosy retrospection, leading individuals to view the past more favorably and the future more negatively. This predisposition influences perceptions and judgments, coloring the outlook on current and future events or trends with a sense of deterioration or loss compared to the pas.

\#\#\#\# Moral luck

Moral luck describes circumstances whereby a moral agent is assigned moral blame or praise for an action or its consequences even if it is clear that said agent did not have full control over either the action or its consequences.

\#\#\#\# Outcome bias

Outcome bias is a cognitive error in evaluating the quality of a decision after the outcome is known. This bias occurs when the same behavior is judged more harshly if it results in a bad outcome, even if the outcome is determined by chance. Essentially, it's the tendency to judge a decision based on its result rather than on the decision-making process itself, often overlooking the role of randomness or external factors in the outcome​​.

\#\#\#\# Hindsight bias

Hindsight bias, also known as the knew-it-all-along effect or creeping determinism, is the inclination, after an event has occurred, to see the event as having been predictable, despite there having been little or no objective basis for predicting it.

\#\#\#\# Rosy retrospection

Rosy retrospection is a psychological phenomenon where people tend to judge the past more positively than the present. This cognitive bias leads to disproportionately favorable views of past events compared to current experiences​​.

\#\#\#\# Telescoping effect

The telescoping effect, also known as the telescoping bias, is a mental error related to memory. It involves the temporal displacement of events, leading people to perceive recent events as being more distant than they actually are (known as backward telescoping) and distant events as being more recent than they are (forward telescoping). This memory error can occur in various situations where individuals make temporal judgments about past events​​.

\#\#\# We think we know what other people are thinking

\#\#\#\# Illusion of transparency

The illusion of transparency is a tendency for people to overestimate the degree to which their personal mental state is known by others. Another way to state this is that people tend to believe their transparent feelings are more apparent to onlookers than is actually the case.

\#\#\#\# Curse of knowledge

The curse of knowledge is a cognitive bias that occurs when an individual, who is communicating with others, assumes that others have the same information that they do, presuming a shared background and understanding. This bias can also be referred to as the curse of expertise, highlighting how those with specialized knowledge or skills may struggle to account for the lack of such knowledge in others​​.

\#\#\#\# Spotlight effect

The spotlight effect is the phenomenon in which people tend to believe they are being noticed more than they really are. Being that one is constantly in the center of one's own world, an accurate evaluation of how much one is noticed by others has shown to be uncommon. The reason behind the spotlight effect comes from the innate tendency to forget that although one is the center of one's own world, one is not the center of everyone else's

\#\#\#\# Extrinsic incentive error

The extrinsic incentives bias is an attributional bias where people tend to attribute more weight to "extrinsic incentives" (like monetary rewards) than to "intrinsic incentives" (such as learning a new skill) when considering the motives of others compared to themselves. This bias represents an exception to the fundamental attribution error, highlighting a tendency to overemphasize external factors over internal motivations when evaluating others' behaviors​.

\#\#\#\# Illusion of external agency

The illusion of external agency (also known as illusion of external control) refers to the tendency for people to perceive themselves as having less agency in their actions than they actually have. For example, people will tend to overestimate the influence of other people or external factors when making a decision, rather than acknowledge the role their own thoughts, judgments and motivations played in that decision.

\#\#\#\# Illusion of asymmetric insight

The illusion of asymmetric insight is a cognitive bias where people perceive their knowledge of others as surpassing those others' knowledge of themselves. This bias reflects a common belief that one's own understanding of other people is deeper or more accurate than their understanding of us​.

\#\#\# We simplify probabilities and numbers to make them easier to think about

\#\#\# Mental accounting

Mental accounting or psychological accounting refers to the tendency of people to separate their money into separate accounts based on a variety of subjective criteria, like the source of the money and intent for each account. According to the theory, individuals assign different functions to each asset group, which has an often irrational and detrimental effect on their consumption and investment decisions.

\#\#\# Appeal to probability fallacy

The appeal to probability fallacy, also known as the appeal to possibility, is a logical fallacy that involves assuming something is true or will definitely occur simply because it might or probably will happen. This fallacy occurs when the possibility of an event is mistaken for certainty, often in the absence of solid evidence. It involves an overreliance on the likelihood of an event, disregarding the need for deductive validity in arguments and often leading to erroneous conclusions based on mere probabilities​​.

\#\#\# Normalcy bias

Normalcy bias, or normality bias, is a tendency for people to fail to prepare for, or adapt to, impending danger which has never materialized in their prior experience or for which they have no framework to interpret as dangerous based on their life experiences so far. This leads to inaction, and to interpret warnings of the danger as overblown.

\#\#\# Murphy’s law

Murphy's law is an adage or epigram commonly stated as "Anything that can go wrong will go wrong." In some versions, it is further extended to include that these mishaps will occur at the worst possible time. This expression captures a kind of fatalistic or pessimistic humor, suggesting that if something has the potential to go wrong, it inevitably will, often under the most inconvenient circumstances​​.

\#\#\# Zero sum bias

Zero-sum bias refers to the intuitively appealing but normally unjustified belief that a situation is like a zero-sum game, in which one person's gain would exactly equal another's loss. Most situations in life are arguably positive-sum games, but the zero-sum heuristic is applied frequently out of habit.

\#\#\# Survivorship bias

Survivorship bias is a logical error that involves focusing on entities that passed a certain selection process, while ignoring those that did not. This bias results in incorrect conclusions due to incomplete data. As a type of selection bias, survivorship bias leads to the disproportionate evaluation of successful outcomes, often neglecting those who did not succeed or "survive" a particular situation. This can result in false or incorrect estimates of probabilities and outcomes, as it skews the perspective toward success stories and away from failures or non-survivors​​.

\#\#\# Subadditivity effect

The subadditivity effect refers to the tendency to judge probability of the whole to be less than the probabilities of the parts. In other words, the likelihood of an event's constituent parts is typically seen as higher than the likelihood of the event itself.

\#\#\# Denomination effect

The denomination effect is a cognitive bias suggesting that people are less likely to spend larger currency denominations compared to their equivalent value in smaller denominations. This bias applies to various forms of currency, where individuals are less inclined to spend money if it is in the form of large bills rather than in smaller bills or coins​​.

\#\#\# Magic number 7+-2

The magical number seven, plus or minus two is a limit proposed in a 1956 paper by psychologist George A. Miller. He summarized research showing that the number of objects an average human can hold in working memory is 7 ± 2\. This is frequently cited in psychology, neuroscience, and design principles as a typical capacity limit of human working memory. In other words, it refers to the finding that human short-term working memory has a capacity of about 7 bits/chunks of information, with the ability to store approximately 7 ± 2 elements. This cognitive science concept has informed theories around the ideal number of options to present people or the optimal amount of information to display at one time without overloading mental bandwidth and comprehension.

\#\#\# We imagine things and people we're familiar with or fond of as better

\#\#\# Out-group homogeneity bias

The out-group homogeneity effect is a cognitive bias where individuals perceive members of an out-group (a group to which they do not belong) as more similar to each other than members of their own in-group. This effect encapsulates the idea expressed in the phrase "they are alike; we are diverse." It highlights a common cognitive distortion where people tend to view members of an out-group as less varied and more homogenous than those in their own group, affecting perceptions and judgments about group diversity and individual differences​​.

\#\#\# Cross-race effect

The cross-race effect, sometimes called cross-race bias or own-race bias, is the tendency to more easily recognize faces that belong to one's own racial group. A well-documented finding is that people are less accurate in remembering and recognizing faces of individuals of a different race than their own.

\#\#\# In-group bias

In-group bias, also known as in-group favoritism, in-group–out-group bias, intergroup bias, or in-group preference, is a psychological pattern where individuals favor members of their own group (in-group) over those from outside groups (out-group). This bias can manifest in various ways, including in the evaluation of others and in the allocation of resources. It reflects a natural tendency to prefer and prioritize the interests, values, and welfare of one's own group over others​​.

\#\#\# Halo effect

The halo effect is a cognitive bias whereby one's judgement of a person's character can be influenced by one's overall (and often first) impression of them, with little actual knowledge about the individual. The term was coined by psychologist Edward Thorndike in 1920 to describe how ratings of one trait will influence ratings of unrelated traits.

\#\#\# Cheerleader effect

In more detail, the cheerleader effect refers to the phenomenon where people tend to find faces more attractive when they are part of a group rather than being viewed individually. Grouping appears to have an "averaging" effect on attractiveness judgments. This cognitive bias causes the same faces to be judged as less attractive when observed individually rather than collectively. The label refers to the common perception that a group of cheerleaders seems more attractive than any individual member alone.

\#\#\# Positivity effect

The positivity effect refers to the ability to constructively analyze situations where desired results are not achieved, yet still garner positive feedback that aids future progression. This cognitive tendency enables individuals to find positive aspects of learning opportunities in experiences that don't necessarily meet their initial expectations or objectives, thus contributing to personal growth and improvement​​.

\#\#\# Not invented here

The Not-Invented-Here (NIH) syndrome is a cognitive bias where individuals or groups show a preference for their own ideas or knowledge over external sources. This bias can hinder knowledge sharing, reuse, and innovation by fostering an attitude that one’s internal ideas are superior. It reflects a natural tendency to prioritize self-generated solutions while rejecting external input, potentially impairing performance. Studies suggest that this bias affects decision-making processes, especially in situations requiring the adoption of existing solutions, with factors such as mindfulness playing a mitigating role. Similar to other biases, NIH can diminish over time through strategies that promote communication, recategorization, and a common-ground approach between internal and external knowledge sources. Understanding this bias is vital for enhancing collaboration, knowledge transfer, and overall performance in organizations and teams.

\#\#\# Reactive devaluation

Reactive devaluation is a cognitive bias where individuals devalue proposals, ideas, or solutions simply because they originate from an opposing party or source. This bias reflects a natural tendency to reject or diminish the worth of external suggestions, especially from perceived adversaries. Reactive devaluation can emerge in various settings, such as political conflicts, negotiations, or disputes, impeding the process of finding mutually beneficial solutions. Beyond the psychological impact, this bias can influence decision-making and economic dynamics, shaping how individuals and organizations perceive and respond to external input.

\#\#\# Well-traveled road effect

The well-traveled road effect is a cognitive bias in which people perceive familiar routes as shorter and quicker than unfamiliar ones, even when travel times and distances are objectively the same. Because familiar paths require less mental effort, attention, and uncertainty, they feel subjectively faster, whereas new or less-known routes feel longer due to greater attentional demand and unpredictability.

\#\#\# We fill in characteristics from stereotypes, generalities, and prior histories

\#\#\# Group attribution error

Group attribution error is a cognitive bias where people assume that the traits or behaviors of one individual from a group reflect the characteristics of the entire group, or conversely, that a group’s overall behavior explains the actions of its individual members. Originating in social psychology, it is closely tied to stereotypes and intergroup bias. The core mechanism is an overgeneralization: humans tend to simplify social information by attributing causes to stable group identities rather than situational factors, which reduces cognitive load but distorts accuracy. For example, if one employee from a particular department misses a deadline, outsiders might assume the whole department is unreliable; or if a company is known to be innovative, an individual employee’s creative idea might be seen as simply a product of the company culture rather than their personal skill.

\#\#\# Ultimate attribution error

Ultimate attribution error is a social psychology bias where people explain negative behavior of outgroup members as stemming from their inherent flaws (dispositional causes), while attributing positive behavior to luck, special circumstances, or exceptions—yet explain ingroup behavior in the opposite way (positive acts as inherent, negative acts as situational). Coined by Thomas F. Pettigrew in 1979, it extends the fundamental attribution error from individuals to entire social groups. The mechanism arises from motivated reasoning and ingroup bias: people want to preserve a positive image of their own group and maintain stereotypes about outgroups, so they distort causal explanations. For example, if a member of a rival political party commits a crime, supporters may say “that’s just how they are,” but if someone in their own party does the same, they might blame external pressures or rare circumstances. Conversely, charitable acts by an outgroup member may be dismissed as self-serving, while similar acts by the ingroup are seen as proof of virtue.

\#\#\# Stereotyping

Stereotyping is the cognitive process of assigning generalized traits, behaviors, or characteristics to all members of a group based on limited information, often ignoring individual differences. In social psychology, stereotyping is seen as a mental shortcut that helps people quickly categorize others, but it frequently leads to inaccurate and biased judgments. The mechanism stems from the brain’s tendency to simplify complex social realities by relying on schemas—mental frameworks built from cultural norms, media, and personal experience—which then become resistant to change. For example, assuming that all engineers are introverted, or that elderly people are technologically incompetent, illustrates stereotyping in everyday life and can shape hiring, education, policing, and social interactions in both subtle and harmful ways.

\#\#\# Essentialism

Essentialism is the belief that certain categories of people, objects, or concepts have an underlying, unchanging “essence” that defines their true nature and determines their characteristics. In psychology and philosophy, it refers to a cognitive bias where humans assume that visible traits or behaviors stem from some deep, immutable property rather than context or experience. The mechanism comes from the mind’s tendency to simplify the world by attributing fixed causes to group identity, which makes reasoning more efficient but often distorts reality. For example, saying “boys are naturally better at math” or “gold is valuable because it has an inherent essence of richness” illustrates essentialist thinking—overlooking cultural, social, or situational influences in favor of assumed inherent qualities.

\#\#\# Functional fixedness

Functional fixedness is a cognitive bias that limits a person’s ability to see new uses for an object beyond its traditional or intended function. In psychology, especially in the study of problem-solving, it is considered a mental set that restricts creative thinking. The mechanism arises from people’s reliance on prior knowledge and learned associations—once an object is mentally labeled for one use, it becomes difficult to imagine alternative applications. For example, if someone needs a paperweight but only sees a hammer as a “tool for nails,” they may overlook its usefulness as a weight to hold papers down; similarly, a box of thumbtacks may be seen only as “a container” rather than as a potential part of the solution in a candle problem experiment.

\#\#\# Moral credential effect

Moral credential effect is a psychological bias where performing (or recalling) a past moral act gives people a kind of “license” to act in ways that might otherwise be seen as immoral, biased, or socially undesirable. Identified in social psychology and behavioral ethics research, it highlights how people balance self-image with behavior: once individuals feel they have proven their virtue, they may feel less constrained about acting in a contradictory way. The mechanism stems from moral self-licensing—maintaining a positive self-concept allows people to rationalize subsequent actions that deviate from their values. For example, someone who donates to charity in the morning might later feel justified in being rude to a coworker, or a company that publicly promotes diversity initiatives may feel freer to overlook discriminatory hiring practices.

\#\#\# Just-world hypothesis

Just-world hypothesis is the cognitive bias where people believe that the world is fundamentally fair, and therefore individuals generally get what they deserve and deserve what they get. Originating in social psychology (Melvin Lerner, 1960s), it reflects a deep psychological need for order and predictability. The mechanism comes from motivated reasoning: by assuming justice is built into reality, people protect themselves from anxiety about random misfortune, but this also leads to victim-blaming and distorted moral judgments. For example, someone might assume that a person facing poverty must have made bad choices, or that a victim of assault somehow provoked it—rationalizations that preserve the belief in a fair world but ignore systemic or situational causes.

\#\#\# Argument from fallacy

Argument from fallacy is a logical error where someone assumes that if an argument contains a fallacy, then its conclusion must be false. In reality, a flawed argument doesn’t automatically invalidate the truth of its conclusion—it only shows that the reasoning used to reach it was faulty. The mechanism stems from confusing the strength of reasoning with the truth of the claim: people dismiss a conclusion entirely because the supporting logic is bad, instead of evaluating the claim on independent evidence. For example, if someone argues “smoking is unhealthy because my uncle told me so,” this is a weak appeal to authority; however, rejecting the conclusion “smoking is unhealthy” solely because the reasoning is fallacious would itself be the argument from fallacy, since independent evidence still shows that smoking is harmful.

\#\#\# Authority bias

Authority bias is the cognitive tendency to give greater weight to the opinions, instructions, or judgments of someone perceived as an authority figure, regardless of the actual evidence or merit of their claims. In psychology and behavioral economics, it is often linked to obedience and conformity studies (e.g., Stanley Milgram’s experiments). The mechanism stems from heuristics and social conditioning: humans are wired to defer to perceived expertise or power as a shortcut for decision-making, which reduces effort but can lead to poor judgment. For example, a patient may accept a doctor’s recommendation without questioning alternative treatments, or employees may follow a CEO’s directive even if it contradicts their own expertise.

\#\#\# Automation bias

Automation bias is the cognitive tendency to over-rely on automated systems, assuming their outputs are more accurate or reliable than they may actually be, even when contradictory evidence or human judgment suggests otherwise. In psychology, human factors research, and safety-critical fields (like aviation and medicine), it’s recognized as a key source of human–automation error. The mechanism stems from trust in technology and cognitive load reduction: people defer to machines because it saves effort and reinforces a perception of objectivity, which can weaken vigilance and critical oversight. For example, a pilot may unquestioningly follow a flawed autopilot instruction, or a doctor may accept a diagnostic AI’s suggestion without verifying against clinical signs—leading to errors that might have been caught with balanced skepticism.

\#\#\# Bandwagon effect

Bandwagon effect is the cognitive bias where people adopt beliefs, behaviors, or trends primarily because they perceive that many others are doing the same, rather than based on their own independent reasoning. In psychology and behavioral economics, it is linked to conformity and herd behavior. The mechanism comes from social proof and the desire for belonging: individuals assume that if “everyone” is doing something, it must be correct, safe, or desirable. For example, investors may rush to buy a stock simply because it’s surging in popularity, or voters may support a political candidate because polls show them leading, creating a self-reinforcing cycle of popularity.

\#\#\# Placebo effect

Placebo effect is the phenomenon where a person experiences a real change in symptoms or outcomes after receiving a treatment with no active therapeutic ingredient, simply because they believe it is effective. In medicine and psychology, it is studied as both a challenge in clinical trials (requiring placebo-controlled groups) and as evidence of the mind–body connection. The mechanism stems from expectation, conditioning, and the brain’s ability to release chemicals (like endorphins or dopamine) that mimic genuine physiological effects when one anticipates healing. For example, a patient given a sugar pill may report reduced pain or improved mood because they believe it is a real drug, or athletes may perform better after drinking what they think is an “energy drink,” even if it’s just flavored water.

\#\#\# We tend to find stories and patterns even when looking at sparse data

\#\#\# Confabulation

Confabulation is the unintentional fabrication of memories, where a person recalls false or distorted information without the intent to deceive, genuinely believing it to be true. In psychology and neuroscience, it is often associated with brain damage (e.g., in Korsakoff’s syndrome, dementia, or after trauma) but can also occur in everyday memory errors. The mechanism arises from the brain’s attempt to fill in gaps in memory or make sense of incomplete information, leading to plausible but inaccurate recollections. For example, a patient with memory impairment may confidently “remember” having attended a family dinner that never happened, or an eyewitness may unknowingly add details to their account of an event that they only inferred or imagined.

\#\#\# Clustering illusion

Clustering illusion is a cognitive bias where people mistakenly perceive patterns, clusters, or streaks in random data, believing they have meaning when in fact they are due to chance. In psychology and statistics, it reflects the human tendency to underestimate randomness and over-interpret coincidences. The mechanism comes from the brain’s pattern-recognition system, which evolved to detect meaningful signals in the environment—even at the cost of seeing false ones—because spotting real patterns was historically adaptive. For example, gamblers may think a slot machine is “hot” after a string of wins, or sports fans may believe a player is “on a streak” when performance variations are actually random noise.

\#\#\# Insensitivity to sample size

Insensitivity to sample size is a cognitive bias where people ignore or underestimate the importance of sample size when evaluating probabilities, risks, or statistical claims. In psychology and decision science, it was highlighted by Tversky and Kahneman in research on judgment under uncertainty. The mechanism comes from reliance on representativeness: people assume that a small sample will reflect the true population just as well as a large one, overlooking the higher variability and error inherent in small samples. For example, someone might believe that a survey of 20 customers is just as reliable as one of 2,000, or assume that a hospital with fewer births should have the same likelihood of daily gender ratios as a much larger hospital, even though small samples fluctuate more dramatically.

\#\#\# Neglect of probability

Neglect of probability is a cognitive bias where people disregard the actual likelihood of events when making judgments or decisions, focusing instead on the vividness, emotional impact, or anecdotal strength of the outcome. In psychology and behavioral economics, it is tied to risk perception and decision-making under uncertainty. The mechanism arises because humans process probabilities poorly—tending to overweight dramatic but rare risks (like plane crashes) and underweight common but mundane ones (like car accidents or heart disease). For example, someone may fear shark attacks far more than slipping in the shower, despite the latter being orders of magnitude more likely, or a policymaker may prioritize resources toward preventing rare disasters while neglecting far more probable public health threats.

\#\#\# Anecdotal fallacy

Anecdotal fallacy is a logical error where someone uses a personal story, isolated case, or vivid example as evidence to prove or disprove a general claim, while ignoring broader data or statistical evidence. In critical thinking and informal logic, it reflects a bias toward emotionally compelling narratives over representative samples. The mechanism comes from the human tendency to find single experiences more memorable and persuasive than abstract numbers, even when they are unrepresentative. For example, arguing that smoking isn’t harmful because “my grandfather smoked daily and lived to 95,” or claiming a product must be effective because “I tried it and it worked for me,” are both anecdotal fallacies that overlook systematic evidence.

\#\#\# Illusion of validity

Illusion of validity is a cognitive bias where people overestimate the accuracy of their judgments, predictions, or evaluations, even when those judgments are based on limited, irrelevant, or statistically weak information. First described by Daniel Kahneman and Amos Tversky in their research on decision-making under uncertainty, it often appears in contexts like hiring, investing, or forecasting. The mechanism stems from reliance on representativeness: when evidence feels consistent with a pattern or stereotype, people become overly confident, ignoring base rates or statistical uncertainty. For example, a recruiter may feel “certain” that a candidate will succeed because their résumé and interview performance fit a familiar success profile, even though such predictions are only weakly correlated with actual job performance.

\#\#\# Masked man fallacy

Masked man fallacy is a logical error that occurs when someone assumes that if two descriptions of a person or thing are not identical, they must refer to different entities. It typically arises from confusing knowledge about something with the identity of that thing. The mechanism comes from a misuse of substitution in intensional contexts (contexts involving belief, knowledge, or perception), where swapping terms that refer to the same entity can change truth value. For example, if someone knows “Clark Kent is a reporter” but doesn’t know “Clark Kent is Superman,” they might falsely conclude that Superman is not a reporter—committing the masked man fallacy by treating ignorance of identity as evidence of non-identity.

\#\#\# Recency illusion

Recency illusion is a cognitive bias where people believe that something they have only just noticed or become aware of is new, when in fact it has existed for a long time. In linguistics, it was coined by Arnold Zwicky to describe how people often assume that a word, phrase, or grammatical construction they’ve recently encountered must be a modern invention. The mechanism stems from selective attention: once something enters our awareness, we notice it more often and mistakenly think it is a recent phenomenon. For example, someone who only just hears people using the word “literally” as an intensifier may think it’s a new misuse, when in fact it has been documented for centuries.

\#\#\# Gambler’s fallacy

Gambler’s fallacy is the mistaken belief that past random events influence the likelihood of future independent events, leading people to expect a “correction” in outcomes. In psychology and behavioral economics, it is a well-documented bias in probability reasoning. The mechanism comes from the representativeness heuristic: people assume short sequences of random outcomes should “look” balanced, so a streak feels as though it must soon reverse. For example, a gambler at a roulette table may believe that after several reds in a row, black is now “due,” even though each spin is independent and the probability remains unchanged.

\#\#\# Hot-hand fallacy

Hot-hand fallacy is the mistaken belief that a person who has experienced success in a random event is more likely to continue succeeding, even though each event is independent. First studied in basketball (where players on a “streak” are thought to be “hot”), it reflects a cognitive bias in probability judgment. The mechanism comes from misperceiving random clusters as evidence of skill momentum—people expect short-term patterns to persist rather than regress to the mean. For example, fans may believe a basketball player who has made several shots in a row is more likely to hit the next one, or investors may assume a stock that’s been rising for days will keep climbing, despite no statistical guarantee.

\#\#\# Illusory correlation

Illusory correlation is the cognitive bias where people perceive a relationship between two variables or events when no actual association exists, or they exaggerate the strength of a weak correlation. In psychology, it helps explain how stereotypes form and persist—individuals notice and remember co-occurrences that confirm expectations while ignoring counterexamples. The mechanism arises from selective attention and memory biases: rare or distinctive events are more salient, making coincidental pairings seem meaningful. For example, someone might believe that members of a minority group are more prone to crime because the combination of “minority status” and “crime” is more memorable, even though data do not support the link; or a gambler might think that carrying a lucky charm correlates with winning, despite outcomes being random.

\#\#\# Pareidolia

Pareidolia is the psychological phenomenon where people perceive meaningful patterns—especially faces or familiar shapes—in random or ambiguous stimuli. In cognitive psychology and neuroscience, it’s considered a byproduct of the brain’s strong pattern-recognition systems, which evolved to detect important signals (like faces) quickly, even at the cost of false positives. The mechanism stems from the brain’s preference for imposing structure on noise, driven by survival advantages of detecting threats or social cues early. For example, people often see faces in clouds, animals in rock formations, or religious figures in toast, even though these images arise purely from random arrangements.

\#\#\# Anthropomorphism

Anthropomorphism is the tendency to attribute human traits, emotions, intentions, or consciousness to non-human entities such as animals, objects, or machines. In psychology, philosophy, and human–computer interaction, it’s recognized as a cognitive bias rooted in humans’ strong social reasoning systems. The mechanism comes from overextending our evolved ability to interpret and predict other people’s behavior—when faced with ambiguous actions from non-humans, we project human-like qualities to make sense of them. For example, pet owners may say their dog feels “guilty” after chewing shoes, or users might believe a voice assistant is “helpful” or “rude,” even though these interpretations reflect human projection rather than actual inner states.